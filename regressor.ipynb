{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79139081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import scipy.ndimage as ndi\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import statistics\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from monai.losses import DiceCELoss,GeneralizedDiceFocalLoss,DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "  \n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    Transposed,\n",
    "    ToTensord,\n",
    "    Resized,\n",
    "    RandLambdad,\n",
    "    SelectItemsd,\n",
    "    Lambdad,\n",
    "    RandRotated,\n",
    "    RandGaussianSmoothd,\n",
    "    RandZoomd,\n",
    "    RandGaussianNoised,\n",
    "    RandScaleIntensityd\n",
    "\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    SmartCacheDataset,\n",
    "    IterableDataset,\n",
    "    ThreadDataLoader,\n",
    "    set_track_meta,\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb47535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms.transform import MapTransform, RandomizableTransform\n",
    "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    "from monai.config import DtypeLike, KeysCollection, SequenceStr\n",
    "class Slice_selector(RandomizableTransform,MapTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        prob: float = 1,\n",
    "        #numslices: int = 8,\n",
    "        #range: tuple = (1,6),\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        MapTransform.__init__(self, keys, allow_missing_keys)\n",
    "        RandomizableTransform.__init__(self, prob)\n",
    "       \n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, torch.Tensor],numslices=8,disrange=(1,6),randomize: bool = True) -> Dict[Hashable, torch.Tensor]:\n",
    "        self.randomize(None)\n",
    "        d = dict(data)\n",
    "\n",
    "        for key in self.key_iterator(d):\n",
    "          shape = d[key].size()\n",
    "          slice_dis = random.randint(*disrange)\n",
    "          span = slice_dis *numslices\n",
    "          while span > shape[0]:\n",
    "            slice_dis = random.randint(*disrange)\n",
    "            span = slice_dis *numslices\n",
    "          min_starting_slice = 0\n",
    "          max_starting_slice = shape[0] - span\n",
    "          starting_slice = random.randint(min_starting_slice, max_starting_slice)\n",
    "          stack = []\n",
    "          d[key] = d[key][starting_slice:starting_slice+numslices*slice_dis:slice_dis,:,:]\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845970f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_FOLDER ='split_img\\\\train\\\\'\n",
    "val_FOLDER ='split_img\\\\val\\\\'\n",
    "test_FOLDER ='split_img\\\\test\\\\'\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "\n",
    "for element in os.listdir(train_FOLDER):\n",
    "  train.append(element)\n",
    "for element in os.listdir(val_FOLDER):\n",
    "  val.append(element)\n",
    "for element in os.listdir(test_FOLDER):\n",
    "  test.append(element)\n",
    "\n",
    "train = [{'image':train_FOLDER+x} for x in train]\n",
    "test = [{'image':test_FOLDER+x} for x in test]\n",
    "val = [{'image':val_FOLDER+x} for x in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        #Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        #Spacingd(\n",
    "        #    keys=[\"image\"],\n",
    "        #    pixdim=(1.5, 1.5, 2.0),\n",
    "        #    mode=(\"bilinear\"),\n",
    "        #),\n",
    "        Resized(\n",
    "            keys=[\"image\"],\n",
    "            spatial_size =(128,128,-1)\n",
    "        ),\n",
    "        #Resized(\n",
    "        #    keys=[\"image\"],\n",
    "        #    spatial_size =(224,224,-1)\n",
    "        #),\n",
    "        RandZoomd(\n",
    "            keys=[\"image\"],\n",
    "            prob=1,\n",
    "            min_zoom=0.8,\n",
    "            max_zoom=1.2,\n",
    "        ),\n",
    "        RandGaussianNoised(\n",
    "            keys=[\"image\"],\n",
    "            prob=0.4,\n",
    "            mean=0.0,\n",
    "            std=0.04,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\"],\n",
    "            spatial_axis=[0],\n",
    "            prob=0.50,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\"],\n",
    "            spatial_axis=[1],\n",
    "            prob=0.50,\n",
    "        ),\n",
    "        RandRotate90d(\n",
    "            keys=[\"image\"],\n",
    "            prob=0.50,\n",
    "            max_k=3,\n",
    "        ),\n",
    "        RandRotated(\n",
    "            keys=[\"image\"],\n",
    "            range_x = .1,\n",
    "            prob=1,\n",
    "        ),\n",
    "        RandScaleIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            factors =0.20,\n",
    "            prob=0.50,\n",
    "        ),\n",
    "        RandShiftIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            offsets=(0,.08),\n",
    "            prob=0.50,\n",
    "        ),\n",
    "        Transposed(keys=[\"image\"],indices=(3,0,1,2)),\n",
    "        Slice_selector(keys=[\"image\"]),\n",
    "        ToTensord(keys=[\"image\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"], ensure_channel_first=True),\n",
    "        Resized(\n",
    "            keys=[\"image\"],\n",
    "            spatial_size =(128,128,-1)\n",
    "        ),\n",
    "        Transposed(keys=[\"image\"],indices=(3,0,1,2)),\n",
    "        ToTensord(keys=[\"image\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb4f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:  11%|â–ˆ         | 32/288 [01:25<12:55,  3.03s/it]"
     ]
    }
   ],
   "source": [
    "train_ds = CacheDataset(data=train, transform=train_transforms, cache_rate=1.0,\n",
    "                        #num_init_workers=4\n",
    "                        )\n",
    "train_loader = ThreadDataLoader(train_ds, num_workers=0, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = CacheDataset(data=val, transform=val_transforms, cache_rate=1.0,\n",
    "                        #num_init_workers=4\n",
    "                        )\n",
    "val_loader = ThreadDataLoader(val_ds, num_workers=0, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25982fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSBR_swin(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc7_res = nn.Linear(1000, 1)\n",
    "        self.model = self.get_swin()\n",
    "    \n",
    "    def get_swin(self):\n",
    "        swin_b = models.swin_t(pretrained=True)\n",
    "        swin_b.features[0][0] = torch.nn.Conv2d(\n",
    "            1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
    "        #swin_b = torch.nn.Sequential(*(list(swin_b.children())[:-1]))\n",
    "        #swin_b.features[-1] = nn.Linear(768, 1)\n",
    "        return swin_b\n",
    "    def forward(self, x: torch.Tensor):\n",
    "      x = F.relu(self.model(x.float()))\n",
    "      x = x.view(-1, 1000)\n",
    "      x = self.fc7_res(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "class SSBR_resnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc7_res = nn.Linear(2048, 1)\n",
    "        self.model = self.get_resnet()\n",
    "    \n",
    "    def get_resnet(self):\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        resnet50.conv1 = torch.nn.Conv2d(\n",
    "            1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "        )\n",
    "        resnet50 = torch.nn.Sequential(*(list(resnet50.children())[:-1]))\n",
    "        return resnet50 \n",
    "    def forward(self, x: torch.Tensor):\n",
    "      x = F.relu(self.model(x.float()))\n",
    "      x = x.view(-1, 2048)\n",
    "      x = self.fc7_res(x)\n",
    "      return x\n",
    "\n",
    "class SSBR_vgg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv6 = nn.Conv2d(\n",
    "            512, 512, 1, stride=1, padding=0\n",
    "        )  # in_channel, out_channel, kernel_size\n",
    "        self.fc7 = nn.Linear(512, 1)\n",
    "        self.model = self.get_vgg()\n",
    "        self.avgpool =nn.AdaptiveAvgPool2d((1,1))\n",
    "    def get_vgg(self):\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        vgg16.features[0] = torch.nn.Conv2d(\n",
    "            1, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        vgg16.to(device)\n",
    "\n",
    "        return vgg16.features\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.model(x.float())\n",
    "        x = F.relu(self.conv6(x))\n",
    "        #x = self.avgpool(x)\n",
    "        x = torch.mean(x, axis=(2, 3))\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "\n",
    "class SSBR_efficientnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc7_res = nn.Linear(1280, 1)\n",
    "        self.model = self.get_efficientnet()\n",
    "    \n",
    "    def get_efficientnet(self):\n",
    "        efficientnet = models.efficientnet_v2_s(pretrained=True)\n",
    "        efficientnet.features[0][0] = torch.nn.Conv2d(\n",
    "            1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
    "        )\n",
    "        efficientnet = torch.nn.Sequential(*(list(efficientnet.children())[:-1]))\n",
    "        return efficientnet\n",
    "    def forward(self, x: torch.Tensor):\n",
    "      x = F.relu(self.model(x.float()))\n",
    "      x = x.view(-1, 1280)\n",
    "      x = self.fc7_res(x)\n",
    "      return x\n",
    "\n",
    "class SSBR_convnext(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc7_res = nn.Linear(768, 1)\n",
    "        self.model = self.get_convnext()\n",
    "    \n",
    "    def get_convnext(self):\n",
    "        convnext = models.convnext_small(pretrained=True)\n",
    "        convnext.features[0] = torch.nn.Conv2d(\n",
    "            1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
    "        convnext = torch.nn.Sequential(*(list(convnext.children())[:-1]))\n",
    "        return convnext\n",
    "    def forward(self, x: torch.Tensor):\n",
    "      x = F.relu(self.model(x.float()))\n",
    "      x = x.view(-1, 768)\n",
    "      x = self.fc7_res(x)\n",
    "      return x\n",
    "\n",
    "class SSBR_densenet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc7_res = nn.Linear(1664, 1)\n",
    "        self.model = self.get_densenet()\n",
    "    \n",
    "    def get_densenet(self):\n",
    "        densenet169 = models.densenet169(pretrained=True)\n",
    "        densenet169.features[0] = torch.nn.Conv2d(\n",
    "            1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "        )\n",
    "        return densenet169.features\n",
    "    def forward(self, x: torch.Tensor):\n",
    "      x = F.relu(self.model(x.float()))\n",
    "      x = torch.mean(x, axis=(2, 3))\n",
    "      x = x.view(-1, 1664)\n",
    "      x = self.fc7_res(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e15ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TimeDistributed(SSBR_efficientnet()).to(device) \n",
    "SSBR_model = \"swin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_order(scores_pred):\n",
    "  scores_diff = scores_pred[:, 1:] - scores_pred[:, :-1]\n",
    "  sigmoid = torch.sigmoid(scores_diff)\n",
    "  loss = -torch.sum(torch.log(sigmoid))\n",
    "  return loss\n",
    "def loss_dist(scores_pred):\n",
    "  scores_diff = scores_pred[:, 1:] - scores_pred[:, :-1]\n",
    "  l1loss = torch.nn.SmoothL1Loss(reduction=\"sum\")\n",
    "  loss = l1loss(scores_diff[:, 1:], scores_diff[:, :-1])\n",
    "  return loss\n",
    "def loss_ssbr(scores_pred,alpha=1):\n",
    "  loss = alpha*loss_order(scores_pred) + loss_dist(scores_pred)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b307dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "global_loss = float('inf')\n",
    "model_path = \"SSBR_experiments\\\\\"+SSBR_model+\".pth\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9281d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    train_loss = 0\n",
    "    for batch_data in train_loader:\n",
    "        model.train() \n",
    "        train_inputs = batch_data[\"image\"].to(device)\n",
    "\n",
    "        train_outputs = model(train_inputs)\n",
    "        #print(train_outputs)\n",
    "        t_loss = loss_ssbr(train_outputs)\n",
    "\n",
    "        train_loss += t_loss\n",
    "        t_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for val_batch_data in val_loader:\n",
    "          v_inputs = val_batch_data[\"image\"].to(device)\n",
    "          v_outputs = model(v_inputs)\n",
    "          #print(v_outputs)\n",
    "          vloss = loss_ssbr(v_outputs)\n",
    "          val_loss += vloss\n",
    "        val_loss /= len(val_loader)\n",
    "    if val_loss <global_loss:\n",
    "        global_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"model was saved\")\n",
    "    else:\n",
    "        print(\"model was not saved\")\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} \")\n",
    "    print(f\"\\nval loss: {val_loss:.5f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df91811",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"], ensure_channel_first=True),\n",
    "        Resized(\n",
    "            keys=[\"image\"],\n",
    "            spatial_size =(128,128,-1)\n",
    "        ),\n",
    "        Transposed(keys=[\"image\"],indices=(3,0,1,2)),\n",
    "        ToTensord(keys=[\"image\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_FOLDER ='split_img\\\\train\\\\'\n",
    "val_FOLDER ='split_img\\\\val\\\\'\n",
    "test_FOLDER ='split_img\\\\test\\\\'\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "\n",
    "for element in os.listdir(train_FOLDER):\n",
    "  train.append(element)\n",
    "for element in os.listdir(val_FOLDER):\n",
    "  val.append(element)\n",
    "for element in os.listdir(test_FOLDER):\n",
    "  test.append(element)\n",
    "\n",
    "train = [{'image':train_FOLDER+x} for x in train]\n",
    "test = [{'image':test_FOLDER+x} for x in test]\n",
    "val = [{'image':val_FOLDER+x} for x in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CacheDataset(data=train, transform=eval_transforms, cache_rate=1.0,\n",
    "                        #num_init_workers=4\n",
    "                        )\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, \n",
    "                          #num_workers=4, \n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CacheDataset(data=test, transform=eval_transforms, cache_rate=1.0,\n",
    "                        #num_init_workers=4\n",
    "                        )\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=True, \n",
    "                          num_workers=4, \n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a3ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = CacheDataset(data=val, transform=eval_transforms, cache_rate=1.0,\n",
    "                        #num_init_workers=4\n",
    "                        )\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=True, \n",
    "                          num_workers=4, \n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TimeDistributed(SSBR_vgg()).to(device) \n",
    "model_name = 'vgg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'SSBRresults\\\\'+model_name+'.pth'\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_json_score(data_loader,output):\n",
    "  torch.load(model_path)\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for batch_data in tqdm(data_loader):\n",
    "      img_path = batch_data[\"image_meta_dict\"][\"filename_or_obj\"][0]\n",
    "      img_name = os.path.split(img_path)[1]\n",
    "      json_name = img_name.split(\".\")[0]+\".json\"\n",
    "      if os.path.exists(output+json_name):\n",
    "        continue\n",
    "      inputs = batch_data[\"image\"].to(device)\n",
    "      outputs = model(inputs)\n",
    "      out_file = {\n",
    "          \"img_name\":img_name,\n",
    "          \"img_path\":img_path,\n",
    "          \"scores\":outputs.squeeze().tolist()\n",
    "      }\n",
    "      out_file = json.dumps(out_file, indent=4)\n",
    "      with open(output+json_name, \"w\") as outfile:\n",
    "        outfile.write(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e946249",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_score(train_loader,'SSBRresults\\\\json\\\\'+model_name+'\\\\train\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d875d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_score(test_loader,'SSBRresults\\\\json\\\\'+model_name+'\\\\test\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa3dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_score(val_loader,'SSBRresults\\\\json\\\\'+model_name+'\\\\val\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"landmark.xlsx\")\n",
    "df=df.drop_duplicates(subset=['filename'])\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bbae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path =  \"spaced_json/train/\"\n",
    "path = 'SSBR_experiments\\\\json\\\\efficientnet\\\\train\\\\'\n",
    "score_df = pd.DataFrame( columns=[\"filename\",\n",
    "                            \"femoral_start\",\"femoral_end\",\n",
    "                            \"bladder_start\",\"bladder_end\",\n",
    "                            \"liver_start\",\"liver_end\",\n",
    "                            \"lung_start\",\"lung_end\",\n",
    "                            \"heart_start\",\"heart_end\",\n",
    "                            \"stomach_start\",\"stomach_end\",\n",
    "                            \"esophagus_start\",\"esophagus_end\",\n",
    "                            \"spleen_start\",\"spleen_end\",\n",
    "                            \"pancreas_start\",\"pancreas_end\",\n",
    "                            \"duodenum_start\",\"duodenum_end\",\n",
    "                            'small_intestine_start','small_intestine_end',\n",
    "                            'large_intestine_start','large_intestine_end',\n",
    "                            'rectum_start','rectum_end',\n",
    "                            'adrenal_start','adrenal_end',\n",
    "                            'gall_bladder_start','gall_bladder_end',\n",
    "                            'kidney_start','kidney_end'])\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "  for filename in filenames:\n",
    "    name = filename.split(\".\")[0]\n",
    "    f = open(dirpath+filename)\n",
    "    data = json.load(f)\n",
    "    scores=data[\"scores\"]\n",
    "    row= df[df['filename']==name]\n",
    "    score_row = {}\n",
    "    for c in score_df.columns[1:]:\n",
    "      if row[c].item()!=-1:\n",
    "        score_row[c]=scores[row[c].item()]\n",
    "    score_row['filename'] = name  \n",
    "    score_df = score_df.append(score_row,ignore_index=True)\n",
    "\n",
    "summary = score_df.describe()\n",
    "transform_min = summary['femoral_start']['mean']\n",
    "transform_max = summary['esophagus_end']['mean']\n",
    "def linear_transform(x, scale=1, min_value=0, max_value=1):\n",
    "    x = x - min_value\n",
    "    x = x * scale / (max_value - min_value)\n",
    "    return x\n",
    "def transform_scores(x):\n",
    "  if (not np.isnan(transform_min)) & (not np.isnan(transform_max)):\n",
    "      transformed = linear_transform(\n",
    "          x,\n",
    "          scale=100,\n",
    "          min_value=transform_min,\n",
    "          max_value=transform_max,\n",
    "      )\n",
    "      return transformed\n",
    "      \n",
    "from scipy.ndimage import gaussian_filter\n",
    "def smooth_scores(x):\n",
    "    smoothed = x.copy()\n",
    "    not_nan = np.where(~np.isnan(x))\n",
    "    not_nan_values = x[not_nan]\n",
    "\n",
    "    # smooth scores\n",
    "    smoothed[not_nan] = gaussian_filter(\n",
    "        not_nan_values, sigma=10 / 1\n",
    "    )\n",
    "\n",
    "    return np.array(smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef45f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'SSBRresults\\\\json\\\\vgg\\\\'\n",
    "for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        f = open(dirpath+\"/\"+filename)\n",
    "        outpath = dirpath+\"/\"+filename\n",
    "        data = json.load(f)\n",
    "        scores=data[\"scores\"]\n",
    "        scores=np.array(scores)\n",
    "        scores = smooth_scores(scores)\n",
    "        scores = transform_scores(scores)\n",
    "        data[\"post_scores\"] = scores.tolist()\n",
    "\n",
    "        out_file = json.dumps(data, indent=4)\n",
    "        with open(outpath, \"w\") as outfile:\n",
    "            outfile.write(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80419892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"landmark.xlsx\")\n",
    "df=df.drop_duplicates(subset=['filename'])\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51982a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_labels ={'Pediatric-CT-SEG-0C78EBBE_img',\n",
    " 'Pediatric-CT-SEG-0D4D6667_img',\n",
    " 'Pediatric-CT-SEG-1D20EAD2_img',\n",
    " 'Pediatric-CT-SEG-23DB510F_img',\n",
    " 'Pediatric-CT-SEG-4A25C4F3_img',\n",
    " 'Pediatric-CT-SEG-66065C99_img',\n",
    " 'Pediatric-CT-SEG-AF83DAF1_img',\n",
    " 'Pediatric-CT-SEG-C9494BA3_img',\n",
    " 'Pediatric-CT-SEG-D1572D48_img',\n",
    " 'Pediatric-CT-SEG-F50AD62F_img'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b05e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_summary(model_name):\n",
    "    post_score_df = pd.DataFrame( columns=[\"filename\",\n",
    "                              \"femoral_start\",\"femoral_end\",\n",
    "                              \"bladder_start\",\"bladder_end\",\n",
    "                              \"liver_start\",\"liver_end\",\n",
    "                              \"lung_start\",\"lung_end\",\n",
    "                              \"heart_start\",\"heart_end\",\n",
    "                              \"stomach_start\",\"stomach_end\",\n",
    "                              \"esophagus_start\",\"esophagus_end\",\n",
    "                              \"spleen_start\",\"spleen_end\",\n",
    "                              \"pancreas_start\",\"pancreas_end\",\n",
    "                              \"duodenum_start\",\"duodenum_end\",\n",
    "                              'small_intestine_start','small_intestine_end',\n",
    "                              'large_intestine_start','large_intestine_end',\n",
    "                              'rectum_start','rectum_end',\n",
    "                              'adrenal_start','adrenal_end',\n",
    "                              'gall_bladder_start','gall_bladder_end',\n",
    "                              'kidney_start','kidney_end'])\n",
    "    path =  \"SSBRresults\\\\json\\\\\"+model_name+\"\\\\train\\\\\"\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            name = filename.split(\".\")[0]\n",
    "            f = open(dirpath+filename)\n",
    "            data = json.load(f)\n",
    "            scores=data[\"post_scores\"]\n",
    "            row= df[df['filename']==name]\n",
    "            score_row = {}\n",
    "            for c in post_score_df.columns[1:]:\n",
    "                if row[c].item()!=-1 and name not in bad_labels:\n",
    "                    #print(filename,c,int(row[c].item()),len(scores))\n",
    "                    score_row[c]=scores[int(row[c].item())]\n",
    "            score_row['filename'] = name  \n",
    "            post_score_df = post_score_df.append(score_row,ignore_index=True)\n",
    "    summary = post_score_df.describe()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = gen_summary(\"efficientnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = np.nanargmin(np.abs(array - value))\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d1196",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_score_df = pd.DataFrame( columns=[\"model\",\"dataset\",\n",
    "                            \"femoral_start\",\"femoral_end\",\n",
    "                            \"bladder_start\",\"bladder_end\",\n",
    "                            \"liver_start\",\"liver_end\",\n",
    "                            \"lung_start\",\"lung_end\",\n",
    "                            \"heart_start\",\"heart_end\",\n",
    "                            \"stomach_start\",\"stomach_end\",\n",
    "                            \"esophagus_start\",\"esophagus_end\",\n",
    "                            \"spleen_start\",\"spleen_end\",\n",
    "                            \"pancreas_start\",\"pancreas_end\",\n",
    "                            \"duodenum_start\",\"duodenum_end\",\n",
    "                            'small_intestine_start','small_intestine_end',\n",
    "                            'large_intestine_start','large_intestine_end',\n",
    "                            'rectum_start','rectum_end',\n",
    "                            'adrenal_start','adrenal_end',\n",
    "                            'gall_bladder_start','gall_bladder_end',\n",
    "                            'kidney_start','kidney_end','overall'])\n",
    "std_score_df = pd.DataFrame( columns=[\"model\",\"dataset\",\n",
    "                            \"femoral_start\",\"femoral_end\",\n",
    "                            \"bladder_start\",\"bladder_end\",\n",
    "                            \"liver_start\",\"liver_end\",\n",
    "                            \"lung_start\",\"lung_end\",\n",
    "                            \"heart_start\",\"heart_end\",\n",
    "                            \"stomach_start\",\"stomach_end\",\n",
    "                            \"esophagus_start\",\"esophagus_end\",\n",
    "                            \"spleen_start\",\"spleen_end\",\n",
    "                            \"pancreas_start\",\"pancreas_end\",\n",
    "                            \"duodenum_start\",\"duodenum_end\",\n",
    "                            'small_intestine_start','small_intestine_end',\n",
    "                            'large_intestine_start','large_intestine_end',\n",
    "                            'rectum_start','rectum_end',\n",
    "                            'adrenal_start','adrenal_end',\n",
    "                            'gall_bladder_start','gall_bladder_end',\n",
    "                            'kidney_start','kidney_end','overall'])\n",
    "models_result_path = \"SSBRresults\\\\json\\\\\"\n",
    "landmark_df = pd.read_excel(\"landmark.xlsx\")\n",
    "for model in os.listdir(models_result_path):\n",
    "  dataset_list = [\"test\",\"train\",\"val\"]\n",
    "  if model==\"totalseg\":\n",
    "      continue\n",
    "  for dataset in dataset_list:\n",
    "    result_path = models_result_path + model +\"\\\\\"+dataset+\"\\\\\"\n",
    "    dic ={}\n",
    "    \n",
    "    model_summary = gen_summary(model)\n",
    "    for (dirpath, dirnames, filenames) in os.walk(result_path):\n",
    "      for filename in filenames:\n",
    "        f = open(dirpath+filename)\n",
    "        name = filename.split(\".\")[0]\n",
    "        data = json.load(f)\n",
    "        scores = data['post_scores']\n",
    "        for landmark in mae_score_df.columns[2:-1]:\n",
    "          start_mean = model_summary[landmark][\"mean\"]\n",
    "          pred =find_nearest(scores,start_mean)\n",
    "          real=landmark_df.loc[landmark_df['filename'] == name.split(\".\")[0]][landmark].values[0]\n",
    "          if real==-1 or name in bad_labels:\n",
    "            continue\n",
    "          factor = 100/len(scores)\n",
    "          scaled_pred = pred*factor\n",
    "          scaled_real = real*factor\n",
    "          mse_score = abs(scaled_pred-scaled_real)\n",
    "          if landmark not in dic:\n",
    "            dic[landmark] = []\n",
    "          dic[landmark].append(mse_score)\n",
    "          if mse_score>1000 and model == 'efficientnet':\n",
    "            print(name,landmark,mse_score,scaled_pred,scaled_real,len(scores))\n",
    "    mean_dic = {}\n",
    "    std_dic = {}\n",
    "    overall_error=[]\n",
    "    for key,value in dic.items():\n",
    "      #mean_dic[key] = sum(value)/len(value)\n",
    "      mean_dic[key] = statistics.mean(value)\n",
    "      std_dic[key] =statistics.stdev(value)\n",
    "      overall_error = overall_error+value\n",
    "      \n",
    "    mean_dic['overall'] = statistics.mean(overall_error)\n",
    "    mean_dic[\"model\"] = model\n",
    "    mean_dic[\"dataset\"] = dataset\n",
    "    \n",
    "    std_dic['overall'] = statistics.stdev(overall_error)\n",
    "    std_dic[\"model\"] = model\n",
    "    std_dic[\"dataset\"] = dataset\n",
    "    mae_score_df = mae_score_df.append(mean_dic,ignore_index=True)\n",
    "    std_score_df = std_score_df.append(std_dic,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ec521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mae_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b73a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_score_df.to_csv(\"mae.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e20598",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_score_df = pd.DataFrame( columns=[\"model\",\"dataset\",\n",
    "                            \"femoral_start\",\"femoral_end\",\n",
    "                            \"bladder_start\",\"bladder_end\",\n",
    "                            \"liver_start\",\"liver_end\",\n",
    "                            \"lung_start\",\"lung_end\",\n",
    "                            \"heart_start\",\"heart_end\",\n",
    "                            \"stomach_start\",\"stomach_end\",\n",
    "                            \"esophagus_start\",\"esophagus_end\",\n",
    "                            \"spleen_start\",\"spleen_end\",\n",
    "                            \"pancreas_start\",\"pancreas_end\",\n",
    "                            \"duodenum_start\",\"duodenum_end\",\n",
    "                            'small_intestine_start','small_intestine_end',\n",
    "                            'large_intestine_start','large_intestine_end',\n",
    "                            'rectum_start','rectum_end',\n",
    "                            'adrenal_start','adrenal_end',\n",
    "                            'gall_bladder_start','gall_bladder_end',\n",
    "                            'kidney_start','kidney_end','overall'])\n",
    "rstd_score_df = pd.DataFrame( columns=[\"model\",\"dataset\",\n",
    "                            \"femoral_start\",\"femoral_end\",\n",
    "                            \"bladder_start\",\"bladder_end\",\n",
    "                            \"liver_start\",\"liver_end\",\n",
    "                            \"lung_start\",\"lung_end\",\n",
    "                            \"heart_start\",\"heart_end\",\n",
    "                            \"stomach_start\",\"stomach_end\",\n",
    "                            \"esophagus_start\",\"esophagus_end\",\n",
    "                            \"spleen_start\",\"spleen_end\",\n",
    "                            \"pancreas_start\",\"pancreas_end\",\n",
    "                            \"duodenum_start\",\"duodenum_end\",\n",
    "                            'small_intestine_start','small_intestine_end',\n",
    "                            'large_intestine_start','large_intestine_end',\n",
    "                            'rectum_start','rectum_end',\n",
    "                            'adrenal_start','adrenal_end',\n",
    "                            'gall_bladder_start','gall_bladder_end',\n",
    "                            'kidney_start','kidney_end','overall'])\n",
    "models_result_path = \"SSBRresults\\\\json\\\\\"\n",
    "landmark_df = pd.read_excel(\"landmark.xlsx\")\n",
    "for model in os.listdir(models_result_path):\n",
    "  if model==\"totalseg\":\n",
    "      continue\n",
    "  dataset_list = [\"test\",\"train\",\"val\"]\n",
    "  for dataset in dataset_list:\n",
    "    result_path = models_result_path + model +\"\\\\\"+dataset+\"\\\\\"\n",
    "    dic ={}\n",
    "    model_summary = gen_summary(model)\n",
    "    for (dirpath, dirnames, filenames) in os.walk(result_path):\n",
    "      for filename in filenames:\n",
    "        f = open(dirpath+filename)\n",
    "        name = filename.split(\".\")[0]\n",
    "        data = json.load(f)\n",
    "        scores = data['post_scores']\n",
    "        for landmark in mse_score_df.columns[2:-1]:\n",
    "          start_mean = model_summary[landmark][\"mean\"]\n",
    "          pred =find_nearest(scores,start_mean)\n",
    "          real=landmark_df.loc[landmark_df['filename'] == name.split(\".\")[0]][landmark].values[0]\n",
    "          if real==-1 or name in bad_labels:\n",
    "            continue\n",
    "          factor = 100/len(scores)\n",
    "          scaled_pred = pred*factor\n",
    "          scaled_real = real*factor\n",
    "          mse_score = ((scaled_pred-scaled_real)**2)\n",
    "          if landmark not in dic:\n",
    "            dic[landmark] = []\n",
    "          dic[landmark].append(mse_score)\n",
    "          if mse_score>1000 and model == 'efficientnet':\n",
    "            print(name,landmark,mse_score,scaled_pred,scaled_real,len(scores))\n",
    "    mean_dic = {}\n",
    "    std_dic = {}\n",
    "    overall_error=[]\n",
    "    for key,value in dic.items():\n",
    "      #mean_dic[key] = sum(value)/len(value)\n",
    "      mean_dic[key] = statistics.mean(value)**(1/2)\n",
    "      std_dic[key] =statistics.stdev(value)**(1/2)\n",
    "      overall_error = overall_error+value\n",
    "      \n",
    "    mean_dic['overall'] = statistics.mean(overall_error)**(1/2)\n",
    "    mean_dic[\"model\"] = model\n",
    "    mean_dic[\"dataset\"] = dataset\n",
    "    \n",
    "    std_dic['overall'] = statistics.stdev(overall_error)**(1/2)\n",
    "    std_dic[\"model\"] = model\n",
    "    std_dic[\"dataset\"] = dataset\n",
    "    rmse_score_df = rmse_score_df.append(mean_dic,ignore_index=True)\n",
    "    rstd_score_df = rstd_score_df.append(std_dic,ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
