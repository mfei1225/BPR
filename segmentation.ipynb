{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import json\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "   \n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    EnsureTyped,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandCropByLabelClassesd,\n",
    "    CenterSpatialCropd,\n",
    "    SpatialPadd,\n",
    "    RandAffined,OneOf,RandGridDistortiond,RandCoarseDropoutd,RandScaleIntensityd,Lambdad\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.networks.nets import SwinUNETR,UNet,UNETR,VNet,DynUNet\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from monai.data import (\n",
    "    ThreadDataLoader,\n",
    "    PersistentDataset,\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    "    SmartCacheDataset,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ped = \"C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\split_img\\\\\" \n",
    "path_total =\"C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\Totalsegmentator_dataset_small_v201\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "removed = {'Pediatric-CT-SEG-0296A78B',\n",
    " 'Pediatric-CT-SEG-03A54FD1',\n",
    " 'Pediatric-CT-SEG-04D3978C',\n",
    " 'Pediatric-CT-SEG-086CF08B',\n",
    " 'Pediatric-CT-SEG-0B387FB5',\n",
    " 'Pediatric-CT-SEG-0C49C1B7',\n",
    " 'Pediatric-CT-SEG-0C5320A6',\n",
    " 'Pediatric-CT-SEG-0C78EBBE',\n",
    " 'Pediatric-CT-SEG-0D4D6667',\n",
    " 'Pediatric-CT-SEG-0D7F3188',\n",
    " 'Pediatric-CT-SEG-0E3FF02E',\n",
    " 'Pediatric-CT-SEG-1069C71E',\n",
    " 'Pediatric-CT-SEG-11D16CF0',\n",
    " 'Pediatric-CT-SEG-1360E26F',\n",
    " 'Pediatric-CT-SEG-1D20EAD2',\n",
    " 'Pediatric-CT-SEG-1F5E00BB',\n",
    " 'Pediatric-CT-SEG-218C084A',\n",
    " 'Pediatric-CT-SEG-23DB510F',\n",
    " 'Pediatric-CT-SEG-2744A45D',\n",
    " 'Pediatric-CT-SEG-27BFD847',\n",
    " 'Pediatric-CT-SEG-2D905107',\n",
    " 'Pediatric-CT-SEG-305BA50D',\n",
    " 'Pediatric-CT-SEG-32C20E78',\n",
    " 'Pediatric-CT-SEG-340BB9AF',\n",
    " 'Pediatric-CT-SEG-34ECBB32',\n",
    " 'Pediatric-CT-SEG-355',\n",
    " 'Pediatric-CT-SEG-356',\n",
    " 'Pediatric-CT-SEG-361304ED',\n",
    " 'Pediatric-CT-SEG-37105B7A',\n",
    " 'Pediatric-CT-SEG-374',\n",
    " 'Pediatric-CT-SEG-380',\n",
    " 'Pediatric-CT-SEG-390',\n",
    " 'Pediatric-CT-SEG-392',\n",
    " 'Pediatric-CT-SEG-393',\n",
    " 'Pediatric-CT-SEG-399',\n",
    " 'Pediatric-CT-SEG-39A80196',\n",
    " 'Pediatric-CT-SEG-3B5CED31',\n",
    " 'Pediatric-CT-SEG-3EE98BA3',\n",
    " 'Pediatric-CT-SEG-402',\n",
    " 'Pediatric-CT-SEG-40210ED4',\n",
    " 'Pediatric-CT-SEG-404',\n",
    " 'Pediatric-CT-SEG-420FD7F5',\n",
    " 'Pediatric-CT-SEG-42BCEEFD',\n",
    " 'Pediatric-CT-SEG-43920D5B',\n",
    " 'Pediatric-CT-SEG-474F00E2',\n",
    " 'Pediatric-CT-SEG-4934405E',\n",
    " 'Pediatric-CT-SEG-4A25C4F3',\n",
    " 'Pediatric-CT-SEG-4A8A81FB',\n",
    " 'Pediatric-CT-SEG-4C1A38AE',\n",
    " 'Pediatric-CT-SEG-51733317',\n",
    " 'Pediatric-CT-SEG-51BBDB38',\n",
    " 'Pediatric-CT-SEG-52AA7DB5',\n",
    " 'Pediatric-CT-SEG-5B1E0A68',\n",
    " 'Pediatric-CT-SEG-5F74CD3E',\n",
    " 'Pediatric-CT-SEG-66065C99',\n",
    " 'Pediatric-CT-SEG-6D6D4CD1',\n",
    " 'Pediatric-CT-SEG-792705D7',\n",
    " 'Pediatric-CT-SEG-7A676F9C',\n",
    " 'Pediatric-CT-SEG-7CE37159',\n",
    " 'Pediatric-CT-SEG-8110AFDA',\n",
    " 'Pediatric-CT-SEG-84CF10F5',\n",
    " 'Pediatric-CT-SEG-8538895B',\n",
    " 'Pediatric-CT-SEG-858CCBEC',\n",
    " 'Pediatric-CT-SEG-885A32DF',\n",
    " 'Pediatric-CT-SEG-8877A5A3',\n",
    " 'Pediatric-CT-SEG-8BB6DC24',\n",
    " 'Pediatric-CT-SEG-8D386469',\n",
    " 'Pediatric-CT-SEG-93718342',\n",
    " 'Pediatric-CT-SEG-9B29D191',\n",
    " 'Pediatric-CT-SEG-9D15A505',\n",
    " 'Pediatric-CT-SEG-9EC8B17D',\n",
    " 'Pediatric-CT-SEG-A00B1583',\n",
    " 'Pediatric-CT-SEG-A1FF5A26',\n",
    " 'Pediatric-CT-SEG-A52B1351',\n",
    " 'Pediatric-CT-SEG-A768A490',\n",
    " 'Pediatric-CT-SEG-A783807D',\n",
    " 'Pediatric-CT-SEG-A79D004C',\n",
    " 'Pediatric-CT-SEG-A975CECD',\n",
    " 'Pediatric-CT-SEG-AC36D24F',\n",
    " 'Pediatric-CT-SEG-ACCA52D8',\n",
    " 'Pediatric-CT-SEG-AF83DAF1',\n",
    " 'Pediatric-CT-SEG-B01E38C9',\n",
    " 'Pediatric-CT-SEG-B0308EC6',\n",
    " 'Pediatric-CT-SEG-B05647FA',\n",
    " 'Pediatric-CT-SEG-B1B6F67A',\n",
    " 'Pediatric-CT-SEG-B1CBA303',\n",
    " 'Pediatric-CT-SEG-B42CD321',\n",
    " 'Pediatric-CT-SEG-BB8C3187',\n",
    " 'Pediatric-CT-SEG-C7338499',\n",
    " 'Pediatric-CT-SEG-C9494BA3',\n",
    " 'Pediatric-CT-SEG-CA967BD7',\n",
    " 'Pediatric-CT-SEG-CF28F28B',\n",
    " 'Pediatric-CT-SEG-D1572D48',\n",
    " 'Pediatric-CT-SEG-D20DA719',\n",
    " 'Pediatric-CT-SEG-D6EED145',\n",
    " 'Pediatric-CT-SEG-DD7F4855',\n",
    " 'Pediatric-CT-SEG-E1FF3C7E',\n",
    " 'Pediatric-CT-SEG-E7702A63',\n",
    " 'Pediatric-CT-SEG-ED076087',\n",
    " 'Pediatric-CT-SEG-F320D54C',\n",
    " 'Pediatric-CT-SEG-F4E60C6E',\n",
    " 'Pediatric-CT-SEG-F50AD62F',\n",
    " 'Pediatric-CT-SEG-FAC9474D',\n",
    " 'Pediatric-CT-SEG-FC6A7B5A'}\n",
    "\n",
    "removed.add('Pediatric-CT-SEG-CF28F28B')\n",
    "len(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = '1'\n",
    "sections = {\n",
    "        \"1\": [\n",
    "            'Femoral Head Rig',\n",
    "            'Femoral Head Lef',\n",
    "            \"Bladder\",\n",
    "            \"Rectum\"\n",
    "        ],\n",
    "        \"0\": [\n",
    "            \"Liver\",\n",
    "            \"Stomach\",\n",
    "            \"Spleen\",\n",
    "            \"Pancreas\",\n",
    "            \"Adrenal Right\",\n",
    "            \"Adrenal Left\",\n",
    "            \"Gall Bladder\",\n",
    "            'Kidney Right',\n",
    "            'Kidney Left',\n",
    "        ],\n",
    "        \"2\": [\n",
    "            'Lung_R',\n",
    "            'Lung_L',\n",
    "            \"Heart\",\n",
    "            \"Esophagus\"\n",
    "        ],\n",
    "        \"3\": [\n",
    "            \"Small Intestine\",\n",
    "            \"Large Intestine\",\n",
    "            \"Duodenum\",\n",
    "            #\"Rectum\"\n",
    "        ],\n",
    "        \"4\": [\n",
    "            'Adrenal Right',\n",
    "            'Adrenal Left',\n",
    "        ],\n",
    "        \"5\": [\n",
    "            \"Pancreas\"\n",
    "        ],\n",
    "    \"7\": [\n",
    "            \"Femoral Head Rig\",\n",
    "            \"Femoral Head Lef\",\n",
    "            \"Bladder\",\n",
    "            \"Rectum\",\n",
    "        #'UteroCervix','Prostate'\n",
    "        ],\n",
    "        \"whole_body\" :[\n",
    "            'Femoral Head Rig',\n",
    "            'Femoral Head Lef',\n",
    "            'Rectum',\n",
    "            'Bladder',\n",
    "            'Large Intestine',\n",
    "            'Small Intestine',\n",
    "            'Duodenum',\n",
    "            'Kidney Right',\n",
    "            'Kidney Left',\n",
    "            'Adrenal Right',\n",
    "            'Adrenal Left',\n",
    "            'Stomach',\n",
    "            'Spleen',\n",
    "            'Pancreas',\n",
    "            'Liver',\n",
    "            'Gall Bladder',\n",
    "            'Lung_R',\n",
    "            'Lung_L',\n",
    "            'Heart',\n",
    "            'Esophagus',\n",
    "        ]\n",
    "    \n",
    "    }\n",
    "\n",
    "oragns={\n",
    " 'Bones': 1,\n",
    " 'Lung_R': 2,\n",
    " 'Stomach': 3,\n",
    " 'Spleen': 4,\n",
    " 'Spinal Canal': 5,\n",
    " 'Small Intestine': 6,\n",
    " 'Rectum': 7,\n",
    " 'Prostate': 8,\n",
    " 'Pancreas': 9,\n",
    " 'Liver': 10,\n",
    " 'Large Intestine': 11,\n",
    " 'Kidney Right': 12,\n",
    " 'Kidney Left': 13,\n",
    " 'Heart': 14,\n",
    " 'Gonads': 15,\n",
    " 'Gall Bladder': 16,\n",
    " 'Femoral Head Rig': 17,\n",
    " 'Femoral Head Lef': 18,\n",
    " 'Esophagus': 19,\n",
    " 'Duodenum': 20,\n",
    " 'Bladder': 21,\n",
    " 'Adrenal Right': 22,\n",
    " 'Adrenal Left': 23,\n",
    " 'Lung_L': 24,\n",
    " 'Thymus': 25,\n",
    " 'UteroCervix': 26,\n",
    " 'Breast Right': 27,\n",
    " 'Breast Left': 28,\n",
    "}\n",
    "\n",
    "\n",
    "nameLbIndMap ={}\n",
    "counter =1\n",
    "for label in sections[seg_model]:\n",
    "    nameLbIndMap[label] = counter\n",
    "    counter +=1\n",
    "\n",
    "inv_label = {v: k for k, v in nameLbIndMap.items()}\n",
    "nameLbIndMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_FOLDER_TRAIN ='C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\split_img\\\\train\\\\'\n",
    "IMG_FOLDER_VAL ='C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\split_img\\\\val\\\\'\n",
    "IMG_FOLDER_TEST ='C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\split_img\\\\test\\\\'\n",
    "LABEL_FOLDER = \"C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\segmentation_label_28\\\\\"\n",
    "def create_dataset(path):\n",
    "    dataset = []\n",
    "    for element in os.listdir(path):\n",
    "        if element.split(\"_\")[0] not in removed:\n",
    "            dataset.append({\n",
    "                \"image\":os.path.join(path,element),\n",
    "                \"label\":os.path.join(LABEL_FOLDER,element.replace(\"_img\", \"_RTseg\")),\n",
    "            })\n",
    "    return dataset\n",
    "train = create_dataset(IMG_FOLDER_TRAIN)\n",
    "val = create_dataset(IMG_FOLDER_VAL)\n",
    "test = create_dataset(IMG_FOLDER_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_labels = list(removed)\n",
    "landmark_df = pd.read_excel(\"C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\landmark.xlsx\")\n",
    "landmark_df=landmark_df.drop_duplicates(subset=['filename'])\n",
    "landmark_df = landmark_df.loc[:, ~landmark_df.columns.str.contains('^Unnamed')]\n",
    "def gen_summary(model_name):\n",
    "    post_score_df = pd.DataFrame( columns=[\"filename\",\n",
    "                              \"femoral_start\",\"femoral_end\",\n",
    "                              \"bladder_start\",\"bladder_end\",\n",
    "                              \"liver_start\",\"liver_end\",\n",
    "                              \"lung_start\",\"lung_end\",\n",
    "                              \"heart_start\",\"heart_end\",\n",
    "                              \"stomach_start\",\"stomach_end\",\n",
    "                              \"esophagus_start\",\"esophagus_end\",\n",
    "                              \"spleen_start\",\"spleen_end\",\n",
    "                              \"pancreas_start\",\"pancreas_end\",\n",
    "                              \"duodenum_start\",\"duodenum_end\",\n",
    "                              'small_intestine_start','small_intestine_end',\n",
    "                              'large_intestine_start','large_intestine_end',\n",
    "                              'rectum_start','rectum_end',\n",
    "                              'adrenal_start','adrenal_end',\n",
    "                              'gall_bladder_start','gall_bladder_end',\n",
    "                              'kidney_start','kidney_end'])\n",
    "    path =  \"C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\SSBR_experiments\\\\json\\\\efficientnet\\\\train\\\\\"\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            name = filename.split(\".\")[0]\n",
    "            f = open(dirpath+filename)\n",
    "            data = json.load(f)\n",
    "            scores=data[\"post_scores\"]\n",
    "            row= landmark_df[landmark_df['filename']==name]\n",
    "            nifti = nib.load(\"H:\\\\monai_ssbr\\\\split_img\\\\train\\\\\" + name +\".nii.gz\")\n",
    "            z_len = nifti.shape[2]\n",
    "            score_row = {}\n",
    "            for c in post_score_df.columns[1:]:\n",
    "                if row[c].item()!=-1 and name not in bad_labels:\n",
    "                    #score_row[c]=scores[int(row[c].item())]\n",
    "                    resampled = int(np.interp(row[c].item(),[0,z_len],[0,len(scores)]))\n",
    "                    score_row[c]=scores[resampled]\n",
    "            score_row['filename'] = name  \n",
    "            post_score_df =pd.concat([post_score_df, pd.DataFrame.from_records([score_row])])\n",
    "            #post_score_df = post_score_df.append(score_row,ignore_index=True)\n",
    "        summary = post_score_df.describe()\n",
    "        return summary\n",
    "model_summary = gen_summary('efficientnet')\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = np.nanargmin(np.abs(array - value))\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms.transform import MapTransform, RandomizableTransform\n",
    "from typing import Any, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    "from monai.config import DtypeLike, KeysCollection, SequenceStr\n",
    "class Ssbr_crop(MapTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        std_range:float,\n",
    "\n",
    "        bounding_organ:str,\n",
    "        min_z_size:int = None,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "      \n",
    "        self.std_range = std_range\n",
    "        self.min_z_size = min_z_size\n",
    "        self.bounding_organ = bounding_organ\n",
    "        MapTransform.__init__(self, keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, torch.Tensor]) -> Dict[Hashable, torch.Tensor]:\n",
    "        d = dict(data)\n",
    "        dirpath = \"C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\SSBR_experiments\\\\json\\\\efficientnet\\\\\"\n",
    "        #print(d.keys())\n",
    "        file = d['image_meta_dict']['filename_or_obj'].split(\"\\\\\")[-1].split('.')[0]+'.json'\n",
    "        #pixdim = d['image_meta_dict']['pixdim'][3]\n",
    "    \n",
    "        f = open(dirpath+file)\n",
    "        ssbr_data = json.load(f)\n",
    "        \n",
    "        scores = ssbr_data['post_scores']\n",
    "        organ_set =set()\n",
    "        for organ in sections[self.bounding_organ]:\n",
    "            if \"Adrenal\" in organ:\n",
    "                organ = 'adrenal'\n",
    "            if \"Kidney\" in organ:\n",
    "                organ = 'kidney'\n",
    "            if \"Femoral\" in organ:\n",
    "                organ = 'Femoral'    \n",
    "            if \"Lung\" in organ:\n",
    "                organ = 'Lung'     \n",
    "            organ = organ.lower()\n",
    "            organ ='_'.join(organ.split(' '))\n",
    "            \n",
    "            if organ+\"_end\" not in model_summary.columns:\n",
    "                continue\n",
    "            if organ+\"_start\" not in model_summary.columns:\n",
    "                continue\n",
    "            organ_set.add((model_summary[organ+\"_start\"][\"mean\"],organ+\"_start\"))\n",
    "            organ_set.add((model_summary[organ+\"_end\"][\"mean\"],organ+\"_end\"))\n",
    "        organ_set = sorted(organ_set, key=lambda tup: tup[0])\n",
    "        start = organ_set[0][1]\n",
    "        end = organ_set[-1][1]\n",
    "        \n",
    "        start_mean = model_summary[start][\"mean\"]\n",
    "        if (start_mean-(self.std_range*model_summary[start][\"std\"])) <0: \n",
    "            start_slice =0\n",
    "        else: \n",
    "            start_slice =find_nearest(scores,start_mean-self.std_range*model_summary[start][\"std\"])\n",
    "       \n",
    "        end_mean = model_summary[end][\"mean\"]\n",
    "        end_slice =find_nearest(scores,end_mean+self.std_range*model_summary[end][\"std\"])\n",
    "        if self.min_z_size:\n",
    "            while (end_slice-start_slice)<self.min_z_size:\n",
    "               \n",
    "                if start_slice >0:\n",
    "                    start_slice-=1\n",
    "                if end_slice<d['image'].shape[3]:\n",
    "                    end_slice +=1\n",
    "        #print(start_slice,end_slice)\n",
    "        d['image'] = d['image'][:,:,:,start_slice:end_slice]\n",
    "        d['label'] = d['label'][:,:,:,start_slice:end_slice]\n",
    "        d['image_meta_dict']['crop_slices'] = (start_slice,end_slice)\n",
    "        #print(d['image_meta_dict']['filename_or_obj'].split(\"\\\\\")[-1].split('.')[0]+'.json')\n",
    "        return d\n",
    "\n",
    "class Relabel(MapTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        sections:list,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.sections=sections\n",
    "        MapTransform.__init__(self, keys, allow_missing_keys)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, torch.Tensor]) -> Dict[Hashable, torch.Tensor]:\n",
    "        d = dict(data)\n",
    "        newlabel =-1\n",
    "        for organ in self.sections:\n",
    "            #print(nameLbIndMap[organ])\n",
    "            d['label'][d['label']==oragns[organ]] = newlabel\n",
    "            newlabel -=1\n",
    "        d['label'][d['label']>0] = 0\n",
    "        d['label'] =  d['label'] *-1\n",
    "        #print(d['image_meta_dict']['filename_or_obj'].split(\"\\\\\")[-1].split('.')[0]+'.json')\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img_size = (48, 48, 48)\n",
    "pix_spacing =(1.5, 1.5, 2.0)\n",
    "std =1.5\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True,image_only=False),\n",
    "        Relabel(keys=[\"label\"],sections=sections[seg_model]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-175,\n",
    "            a_max=250,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        #Lambdad(keys=[\"image\"], func=lambda x: x / x.max()),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=pix_spacing,\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        SpatialPadd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_size=img_size,\n",
    "        ),\n",
    "        #Ssbr_crop(keys=[\"image\", \"label\",],std_range=std,min_z_size=img_size[2],bounding_organ = seg_model),#,min_z_size=img_size[2]\n",
    "       \n",
    "        RandSpatialCropSamplesd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            num_samples=num_samples,\n",
    "            roi_size=img_size,\n",
    "            random_size=False\n",
    "        ), \n",
    "       \n",
    "        EnsureTyped(keys=(\"image\", \"label\"), dtype=torch.float32),\n",
    "     \n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True,image_only=False),\n",
    "        \n",
    "        Relabel(keys=[\"label\",],sections=sections[seg_model]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True\n",
    "        ),\n",
    "        #Lambdad(keys=[\"image\"], func=lambda x: x / x.max()),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=pix_spacing,\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        SpatialPadd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_size=img_size,\n",
    "        ),\n",
    "        EnsureTyped(keys=(\"image\", \"label\"), dtype=torch.float32),\n",
    "        #Ssbr_crop(keys=[\"image\", \"label\",],std_range=1,min_z_size=img_size[2],bounding_organ = seg_model),\n",
    "        #CenterSpatialCropd(keys=[\"image\", \"label\",],roi_size =img_size),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PersistentDataset(\n",
    "    data=train,\n",
    "    transform=train_transforms,\n",
    "    cache_dir=\"cache\",\n",
    ")\n",
    "train_loader = ThreadDataLoader(train_ds, num_workers=0, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = PersistentDataset(\n",
    "    data=val,\n",
    "    transform=val_transforms,\n",
    "    cache_dir=\"cache_val\",\n",
    ")\n",
    "val_loader = ThreadDataLoader(val_ds, num_workers=0, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = train_ds[0]\n",
    "i,l= check_data[0][\"image\"],check_data[0][\"label\"]\n",
    "print(i.shape)\n",
    "plt.imshow(i[0, :, :, 24].detach().cpu(), cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(l[0, :, :, 24].detach().cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernels_strides(sizes, spacings):\n",
    "    \"\"\"\n",
    "    This function is only used for decathlon datasets with the provided patch sizes.\n",
    "    When refering this method for other tasks, please ensure that the patch size for each spatial dimension should\n",
    "    be divisible by the product of all strides in the corresponding dimension.\n",
    "    In addition, the minimal spatial size should have at least one dimension that has twice the size of\n",
    "    the product of all strides. For patch sizes that cannot find suitable strides, an error will be raised.\n",
    "    \"\"\"\n",
    "    input_size = sizes\n",
    "    strides, kernels = [], []\n",
    "    while True:\n",
    "        spacing_ratio = [sp / min(spacings) for sp in spacings]\n",
    "        stride = [2 if ratio <= 2 and size >= 8 else 1 for (ratio, size) in zip(spacing_ratio, sizes)]\n",
    "        kernel = [3 if ratio <= 2 else 1 for ratio in spacing_ratio]\n",
    "        if all(s == 1 for s in stride):\n",
    "            break\n",
    "        for idx, (i, j) in enumerate(zip(sizes, stride)):\n",
    "            if i % j != 0:\n",
    "                raise ValueError(\n",
    "                    f\"Patch size is not supported, please try to modify the size {input_size[idx]} in the spatial dimension {idx}.\"\n",
    "                )\n",
    "        sizes = [i / j for i, j in zip(sizes, stride)]\n",
    "        spacings = [i * j for i, j in zip(spacings, stride)]\n",
    "        kernels.append(kernel)\n",
    "        strides.append(stride)\n",
    "\n",
    "    strides.insert(0, len(spacings) * [1])\n",
    "    kernels.append(len(spacings) * [3])\n",
    "    return kernels, strides\n",
    "kernels, strides = get_kernels_strides((48, 48, 48),pix_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameLbIndMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels =len(nameLbIndMap)+1\n",
    "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DynUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=out_channels,\n",
    "    kernel_size=kernels,\n",
    "    strides=strides,\n",
    "    upsample_kernel_size=strides[1:],\n",
    ").to(device)\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 500\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=out_channels)])\n",
    "post_label = Compose([AsDiscrete(to_onehot=out_channels)])\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "model_name = \"RNSA\\\\final\\\\section_\"+seg_model+\"_whole.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_label = AsDiscrete(to_onehot=out_channels)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=out_channels)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "def validation(epoch_iterator_val,is_cpu = False):\n",
    "    model.eval()\n",
    "    dice_metric.reset()\n",
    "    with torch.no_grad():\n",
    "            for val_data in epoch_iterator_val:\n",
    "                if is_cpu:\n",
    "                    val_inputs, val_labels = (\n",
    "                        val_data[\"image\"].to(device),\n",
    "                        val_data[\"label\"],\n",
    "                    )\n",
    "                else:\n",
    "                    val_inputs, val_labels = (\n",
    "                        val_data[\"image\"].to(device),\n",
    "                        val_data[\"label\"].to(device),\n",
    "                    )\n",
    "\n",
    "                sw_batch_size = 1\n",
    "                if is_cpu:\n",
    "                    val_outputs = sliding_window_inference(\n",
    "                        val_inputs, img_size, sw_batch_size, model,device=torch.device('cpu'))\n",
    "                else:\n",
    "                    val_outputs = sliding_window_inference(\n",
    "                        val_inputs, img_size, sw_batch_size, model)\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                \n",
    "                # compute metric for current iteration\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "            organ_dice=dice_metric.get_buffer().cpu().numpy()\n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            # reset the status for next validation round\n",
    "    return metric,organ_dice\n",
    "\n",
    "def eval_ssbr_crop(file,bounding_organ,std_range,min_z_size):\n",
    "    dirpath = \"C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\SSBR_experiments\\\\json\\\\efficientnet\\\\\"\n",
    "    file = file.split(\"\\\\\")[-1].split('.')[0]+'.json'        \n",
    "    f = open(dirpath+file)\n",
    "    ssbr_data = json.load(f)\n",
    "    scores = ssbr_data['post_scores']\n",
    "    organ_set =set()\n",
    "    for organ in sections[bounding_organ]:\n",
    "        if \"Adrenal\" in organ:\n",
    "                organ = 'adrenal'\n",
    "        if \"Kidney\" in organ:\n",
    "                organ = 'kidney'\n",
    "        if \"Femoral\" in organ:\n",
    "                organ = 'Femoral'\n",
    "        if \"Lung\" in organ:\n",
    "                organ = 'Lung' \n",
    "        organ = organ.lower()\n",
    "        organ ='_'.join(organ.split(' '))\n",
    "        organ_set.add((model_summary[organ+\"_start\"][\"mean\"],organ+\"_start\"))\n",
    "        organ_set.add((model_summary[organ+\"_end\"][\"mean\"],organ+\"_end\"))\n",
    "    organ_set = sorted(organ_set, key=lambda tup: tup[0])\n",
    "    start = organ_set[0][1]\n",
    "    end = organ_set[-1][1]\n",
    "    start_mean = model_summary[start][\"mean\"]\n",
    "    if (start_mean-(std_range*model_summary[start][\"std\"])) <0: \n",
    "        start_slice =0\n",
    "    else: \n",
    "        start_slice =find_nearest(scores,start_mean-std_range*model_summary[start][\"std\"])\n",
    "    end_mean = model_summary[end][\"mean\"]\n",
    "    end_slice =find_nearest(scores,end_mean+std_range*model_summary[end][\"std\"])\n",
    "    while (end_slice-start_slice)<=min_z_size:\n",
    "        #print(start_slice,end_slice,min_z_size)\n",
    "        if start_slice >0:\n",
    "            start_slice-=1\n",
    "        if end_slice<=len(scores) or end_slice<=min_z_size:\n",
    "            end_slice +=1\n",
    "    return start_slice,end_slice\n",
    "\n",
    "def validation_crop(epoch_iterator_val):\n",
    "    model.eval()\n",
    "    dice_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for data in epoch_iterator_val:\n",
    "            start_slice,end_slice =eval_ssbr_crop(\n",
    "                data[\"image_meta_dict\"]['filename_or_obj'][0],\n",
    "                seg_model,\n",
    "                std,\n",
    "                img_size[2],\n",
    "            ) \n",
    "            #print(val_data[\"image\"].shape,val_data[\"label\"].shape,start_slice,end_slice)\n",
    "            crop_inputs, crop_labels = (\n",
    "                data[\"image\"][:,:,:,:,start_slice:end_slice].to(device),\n",
    "                data[\"label\"][:,:,:,:,start_slice:end_slice].to(device),\n",
    "            )\n",
    "         \n",
    "            sw_batch_size = 1\n",
    "            crop_outputs = sliding_window_inference(\n",
    "                crop_inputs, img_size, sw_batch_size, model)\n",
    "            post_outputs = [post_pred(i) for i in decollate_batch(crop_outputs)]\n",
    "            post_labels = [post_label(i) for i in decollate_batch(crop_labels)]\n",
    "            #print(val_outputs[0].shape)\n",
    "            label =[post_label(i) for i in decollate_batch(data[\"label\"])]\n",
    "            pred = torch.zeros(*label[0].shape)\n",
    "            pred[0,:,:,:] = 1\n",
    "            #print(pred[:,:,:,start_slice:end_slice].shape,val_outputs[0].shape)\n",
    "            pred[:,:,:,start_slice:end_slice] = post_outputs[0]\n",
    "            \n",
    "            # compute metric for current iteration\n",
    "            dice_metric(y_pred=[pred], y=label)\n",
    "            #print(dice_metric.get_buffer().cpu().numpy())\n",
    "\n",
    "        metric = dice_metric.aggregate().item()\n",
    "        organ_dice=dice_metric.get_buffer().cpu().numpy()\n",
    "        \n",
    "    return metric,organ_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        x, y  = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        #print(x.shape,y.shape)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logit_map = model(x)\n",
    "            loss = loss_function(logit_map, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        epoch_loss += loss.item()\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "        #print(\n",
    "        #    f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "        #    f\"train_loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        metric,organ_dice= validation(val_loader)\n",
    "        #metric,organ_dice = validation_crop(val_loader)\n",
    "        metric_values.append(metric)\n",
    "        print(organ_dice)\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_metric_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), os.path.join(\n",
    "                 model_name))\n",
    "            print(\"saved new best metric model\")\n",
    "            #print(dice_metric.get_buffer().cpu().numpy())\n",
    "        print(\n",
    "            f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "            f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "            f\"at epoch: {best_metric_epoch}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True),\n",
    "        \n",
    "        Relabel(keys=[\"label\",],sections=sections[seg_model]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True\n",
    "        ),\n",
    "   \n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        SpatialPadd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_size=img_size,\n",
    "        ),\n",
    "        EnsureTyped(keys=(\"image\", \"label\"), dtype=torch.float32),\n",
    "        #Ssbr_crop(keys=[\"image\", \"label\",],std_range=1.5,min_z_size=img_size[2],bounding_organ = seg_model),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_ds = CacheDataset(\n",
    "    data=test, transform=val_transforms, cache_rate=1.0#num_workers=4\n",
    ")\n",
    "test_loader = ThreadDataLoader(test_ds, num_workers=0, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric,organ_dice= validation(test_loader)\n",
    "mean_organ = np.average(organ_dice,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sections[seg_model])):\n",
    "  print(sections[seg_model][i] +\": \"+ str(mean_organ[i+1]))\n",
    "print(\"Overall Dice: \" + str(metric))\n",
    "dice_metric.get_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric,organ_dice= validation_crop(test_loader)\n",
    "mean_organ = np.average(organ_dice,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sections[seg_model])):\n",
    "  print(sections[seg_model][i] +\": \"+ str(mean_organ[i+1]))\n",
    "print(\"Overall Dice: \" + str(metric))\n",
    "dice_metric.get_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_organ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "data = [0.9689,\n",
    "0.9736,\n",
    "0.969,\n",
    "0.9746,\n",
    "0.9723,\n",
    "0.9723,\n",
    "0.9759,\n",
    "0.9733,\n",
    "0.9664,\n",
    "0.946,\n",
    "0.9756,\n",
    "0.9701,\n",
    "0.9697,\n",
    "0.9742,\n",
    "0.9741,\n",
    "0.9694,\n",
    "0.9725,\n",
    "0.9734,\n",
    "0.9639,\n",
    "0.9683,\n",
    "0.9397,\n",
    "0.9638,\n",
    "0.9715,\n",
    "0.9656,\n",
    "0.9599,\n",
    "0.9499,\n",
    "0.9815,\n",
    "\n",
    "]\n",
    "\n",
    "# Calculate the sample mean and standard error\n",
    "mean = np.mean(data)\n",
    "sem = stats.sem(data)  # Standard Error of the Mean\n",
    "confidence_level = 0.95\n",
    "degrees_freedom = len(data) - 1\n",
    "confidence_interval = stats.t.interval(confidence_level, degrees_freedom, loc=mean, scale=sem)\n",
    "# Display the results\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_organ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\split_img\\\\test\\\\\"\n",
    "for p in os.listdir(path):\n",
    "    img = nib.load(os.path.join(path,p))\n",
    "    header = img.header\n",
    "    print(header)\n",
    "\n",
    "    # Scanner type may not be directly listed, but you can try looking for it\n",
    "    # in auxiliary fields (this depends on the data).\n",
    "    # For example, some scanner information could be in the description field:\n",
    "    scanner_info = header.get('descrip', 'No scanner information found')\n",
    "\n",
    "    print(\"Scanner type:\", scanner_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "path = \"C:\\\\Users\\\\Michael\\\\Documents\\\\monai_ssbr\\\\split_img\\\\test\\\\\"\n",
    "for p in os.listdir(path):\n",
    "    s=p.split(\"_\")[0]\n",
    "    path_s = \"C:\\\\Users\\\\Michael\\\\Desktop\\\\manifest-1647979711903\\\\Pediatric-CT-SEG\\\\\" +s\n",
    "    for f in  os.listdir(os.path.join(path_s)):\n",
    "        for file in  os.listdir(os.path.join(path_s,f)):\n",
    "            \n",
    "            if \"-CT\" in file:\n",
    "                d = os.listdir(os.path.join(path_s,f,file))[0]\n",
    "                print(d)\n",
    "                print(os.path.join(path_s,f,file,d))\n",
    "                dicom_data = pydicom.dcmread(os.path.join(path_s,f,file,d))\n",
    "                manufacturer = dicom_data.get('Manufacturer', 'Unknown')\n",
    "                model = dicom_data.get('ManufacturerModelName', 'Unknown')\n",
    "                station_name = dicom_data.get('StationName', 'Unknown')\n",
    "                software_version = dicom_data.get('SoftwareVersions', 'Unknown')\n",
    "                if model not in models:\n",
    "                    models[model]= []\n",
    "                models[model].append(s)\n",
    "                print(f\"Manufacturer: {manufacturer}\")\n",
    "                print(f\"Model: {model}\")\n",
    "                print(f\"Station Name: {station_name}\")\n",
    "                print(f\"Software Version: {software_version}\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'LightSpeed VCT': ['Pediatric-CT-SEG-050CB44A',\n",
    "  'Pediatric-CT-SEG-0C78EBBE',\n",
    "  'Pediatric-CT-SEG-11FCDCBD',\n",
    "  'Pediatric-CT-SEG-1644392C',\n",
    "  'Pediatric-CT-SEG-1D20EAD2',\n",
    "  'Pediatric-CT-SEG-3B5CED31',\n",
    "  'Pediatric-CT-SEG-491B83F3',\n",
    "  'Pediatric-CT-SEG-4A25C4F3',\n",
    "  'Pediatric-CT-SEG-4B20AED7',\n",
    "  'Pediatric-CT-SEG-51733317',\n",
    "  'Pediatric-CT-SEG-57A2E054',\n",
    "  'Pediatric-CT-SEG-59397A22',\n",
    "  'Pediatric-CT-SEG-66EE3D8A',\n",
    "  'Pediatric-CT-SEG-769F1A5A',\n",
    "  'Pediatric-CT-SEG-968A78B3',\n",
    "  'Pediatric-CT-SEG-AB8F282E',\n",
    "  'Pediatric-CT-SEG-F0386168',\n",
    "  'Pediatric-CT-SEG-F50AD62F'],\n",
    " 'SOMATOM Definition AS+': ['Pediatric-CT-SEG-0C5320A6',\n",
    "  'Pediatric-CT-SEG-1188A817',\n",
    "  'Pediatric-CT-SEG-20A9D6EC',\n",
    "  'Pediatric-CT-SEG-354',\n",
    "  'Pediatric-CT-SEG-380',\n",
    "  'Pediatric-CT-SEG-381',\n",
    "  'Pediatric-CT-SEG-3B10B8AB',\n",
    "  'Pediatric-CT-SEG-400',\n",
    "  'Pediatric-CT-SEG-5B1E0A68',\n",
    "  'Pediatric-CT-SEG-677673CD',\n",
    "  'Pediatric-CT-SEG-691FB79F',\n",
    "  'Pediatric-CT-SEG-826600AF',\n",
    "  'Pediatric-CT-SEG-84CF10F5',\n",
    "  'Pediatric-CT-SEG-906B8910',\n",
    "  'Pediatric-CT-SEG-920AD27A',\n",
    "  'Pediatric-CT-SEG-A768A490'],\n",
    " 'Revolution CT': ['Pediatric-CT-SEG-3DAE6E6F',\n",
    "  'Pediatric-CT-SEG-627003CD',\n",
    "  'Pediatric-CT-SEG-7C205617',\n",
    "  'Pediatric-CT-SEG-7E9377C4',\n",
    "  'Pediatric-CT-SEG-AF83DAF1']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "path = \"C:\\\\Users\\\\Michael\\\\Desktop\\\\manifest-1647979711903\\\\Pediatric-CT-SEG\\\\Pediatric-CT-SEG-00DCF4D6\\\\10-09-2009-NA-CT-45894\\\\30144.000000-CT-67414\\\\1-003.dcm\"\n",
    "\n",
    "dicom_data = pydicom.dcmread(path)\n",
    "\n",
    "# Extract scanner-related information\n",
    "manufacturer = dicom_data.get('Manufacturer', 'Unknown')\n",
    "model = dicom_data.get('ManufacturerModelName', 'Unknown')\n",
    "station_name = dicom_data.get('StationName', 'Unknown')\n",
    "software_version = dicom_data.get('SoftwareVersions', 'Unknown')\n",
    "print(f\"Manufacturer: {manufacturer}\")\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"Station Name: {station_name}\")\n",
    "print(f\"Software Version: {software_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
